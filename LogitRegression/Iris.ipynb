{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris\n",
    "Dataset Iris is a fine example to run Logit Regression, which is another algorithm of ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, we use the same method in linear regression. In linear model, we predict label $\\hat{y_i} \\in \\mathbb{R}$ for each $x_i\\in D,x_i = (x_1,x_2,\\cdots,x_n)$. And, our model is linear, so\n",
    "$$\\hat{y_i} = w_0 + w_1x_{i1} + \\cdots w_nx_{in}$$\n",
    "Through LSM or Gradient Descent(BGD,SGD or mini-BGD) learning on $D$ and labelset $Y$, we can get to the parameter $\\hat{w} = (w_0,w_1,\\cdots,w_n)^T$ to predict. Let $\\hat{x_i} = (1, x_i1, x_i2,\\cdots,x_in)$,then $\\hat{y_i} = \\hat{x_i}\\hat{w}$. For each $x_i$(such $x_i$ total sum is $m$, which is $|D|$), let $X$ be\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and $\\hat{y}$ be $(\\hat{y_1},\\hat{y_2},\\cdots,\\hat{y_m})^T$, then\n",
    "$$\\hat{y} = X\\hat{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the linear regression model, but for logistic regression the label $y_i \\in \\{0,1\\}$, how to correct the output to fit the new label?\n",
    "One method is to use Sigmoid function $\\sigma(z) = \\frac{1}{1 + \\exp{(-z)}}$.\n",
    "Let what we predict in logistic regression is the possibility $\\hat{y_i} = \\sigma(\\hat{x_i}\\hat{w}) = \\frac{1}{1 + \\exp(\\hat{x_i}\\hat{w})}$, and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y_i}\n",
    "\\begin{cases}\n",
    "\\geq 0.5 , y_i = 1 \\\\\n",
    "< 0.5 , y_i = 0 \n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Now we know $\\hat{y_i} = p(y_i = 1|\\hat{x_i},\\hat{w})$,for $\\hat{y_i} = \\sigma({\\hat{x_i}\\hat{w}})$, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\hat{y_i} = \\sigma({\\hat{x_i}\\hat{w}}) \\Rightarrow \\\\\n",
    "    & ln{\\frac{\\hat{y_i}}{1 - \\hat{y_i}}} = \\hat{x_i}\\hat{w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $1 - \\hat{y_i}$ is $p(y_i = 0|\\hat{x_i},\\hat{w})$, $\\frac{\\hat{y_i}}{1 - \\hat{y_i}} = \\frac{p(y_i = 1|\\hat{x_i},\\hat{w})}{p(y_i = 0|\\hat{x_i},\\hat{w})}$ reveals the odds of $y_i = 1$.\n",
    "Then we use Entropy as loss function.$l(\\hat{w}) = y_iln(p(y_i = 1|\\hat{x_i},\\hat{w})) + (1 - y_i)p(y_i = 0|\\hat{x_i},\\hat{w})$.The Cost function is\n",
    "$$\n",
    "J(\\hat{w}) = -\\frac{1}{m}\\sum_{i = 1}^{m}l(\\hat{w}) + \\frac{\\lambda}{2m}\\sum_{j = 0}^{n}w_j^2\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is GD.\n",
    "$$\n",
    "\\hat{w}^* = \\argmin_{\\hat{w}} J(\\hat{w})\n",
    "$$\n",
    "And\n",
    "$$\n",
    "\\hat{w}^{t + 1} = \\hat{w}^t - \\alpha\\frac{\\partial J(\\hat{w}^t)}{\\partial\\hat{w}^t}\n",
    "$$\n",
    "where $\\alpha$ is learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        12\\n           1       1.00      1.00      1.00         8\\n\\n    accuracy                           1.00        20\\n   macro avg       1.00      1.00      1.00        20\\nweighted avg       1.00      1.00      1.00        20\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "accuracy \n",
    "conf_matrix\n",
    "class_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
